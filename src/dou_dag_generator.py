"""
Dynamic DAG generator with YAML config system to create DAG which
searches terms in the Gazzete [Diário Oficial da União-DOU] and send it
by email to the  provided `recipient_emails` list. The DAGs are
generated by YAML config files at `dag_confs` folder.

TODO:
[] - Definir sufixo do título do email a partir de configuração
"""

import ast
import logging
import os
import sys
import textwrap
from dataclasses import asdict
from datetime import datetime, timedelta
from typing import Dict, List

import pandas as pd
from airflow import DAG
from airflow.hooks.base import BaseHook
from airflow.operators.empty import EmptyOperator
from airflow.operators.python import BranchPythonOperator, PythonOperator
from airflow.providers.microsoft.mssql.hooks.mssql import MsSqlHook
from airflow.providers.postgres.hooks.postgres import PostgresHook

from FastETL.custom_functions.utils.date import (
    get_trigger_date, template_ano_mes_dia_trigger_local_time)

sys.path.insert(0, os.path.abspath(os.path.dirname(__file__)))

from notification.notifier import Notifier
from parsers import DAGConfig, YAMLParser
from searchers import BaseSearcher, DOUSearcher, QDSearcher
from util import get_source_dir


YAMLS_START_DIR = os.path.join(get_source_dir(), 'dag_confs')

class DouDigestDagGenerator():
    """
    YAML based Generator of DAGs that digests the DOU (gazette) daily
    publication and send email report listing all documents matching
    pré-defined keywords. It's also possible to fetch keywords from the
    database.
    """

    parser = YAMLParser
    searchers: Dict[str, BaseSearcher]

    def __init__(self, on_retry_callback=None, on_failure_callback=None):
        self.searchers = {
            'DOU': DOUSearcher(),
            'QD': QDSearcher(),
        }
        self.on_retry_callback = on_retry_callback
        self.on_failure_callback = on_failure_callback

    @staticmethod
    def prepare_doc_md(specs: DAGConfig, config_file: str) -> str:
        """Prepares the markdown documentation for a dag.

        Args:
            specs (DAGConfig): A DAG configuration object.
            config_file (str): The name of a DAG config file.

        Returns:
            str: The DAG documentation in markdown format.
        """
        config = asdict(specs)
        # options that won't show in the "DAG Docs"
        del config["description"]
        del config["doc_md"]
        doc_md = (
            specs.doc_md +
            textwrap.dedent(f"""

            **Configuração da dag definida no arquivo `{config_file}`**:

            <dl>
            """
            )
        )
        for key, value in config.items():
            doc_md = doc_md + f"<dt>{key}</dt>"
            if isinstance(value, list) or isinstance(value, set):
                doc_md = doc_md + (
                    "<dd>\n\n" +
                    " * " +
                    "\n * ".join(str(item) for item in value) +
                    "\n</dd>"
                )
            else:
                doc_md = doc_md + f"<dd>{str(value)}</dd>"
            doc_md = doc_md + "\n"
        doc_md = doc_md + "</dl>\n"
        return doc_md

    def find_yml_files(self, directory):
        """
        Recursively find all .yml and .yaml files inside a directory.

        Args:
            directory (str): The path to the directory to search in.

        Returns:
            list: A list of paths to .yml and .yaml files found inside
                the directory.
        """

        yml_files = []
        for root, dirs, files in os.walk(directory):
            for file in files:
                if file.endswith(".yml") or file.endswith(".yaml"):
                    yml_files.append(os.path.join(root, file))

        return yml_files

    def generate_dags(self):
        """Iterates over the YAML files and creates all dags
        """

        files_list = self.find_yml_files(YAMLS_START_DIR)

        for file_path in files_list:
            dag_specs = self.parser(file_path).parse()
            dag_id = dag_specs.dag_id
            globals()[dag_id] = self.create_dag(
                dag_specs,
                os.path.basename(file_path)
            )

    def create_dag(self, specs: DAGConfig, config_file: str) -> DAG:
        """Creates the DAG object and tasks

        Depending on configuration it adds an extra prior task to query
        the term_list from a database
        """
        # Prepare the markdown documentation
        doc_md = self.prepare_doc_md(
            specs, config_file) if specs.doc_md else specs.doc_md
        # DAG parameters
        default_args = {
            'owner': 'nitai',
            'start_date': datetime(2021, 10, 18),
            'depends_on_past': False,
            'retries': 10,
            'retry_delay': timedelta(minutes=20),
            'on_retry_callback': self.on_retry_callback,
            'on_failure_callback': self.on_failure_callback,
        }
        dag = DAG(
            specs.dag_id,
            default_args=default_args,
            schedule=specs.schedule,
            description=specs.description,
            doc_md=doc_md,
            catchup=False,
            params={
                "trigger_date": "2022-01-02T12:00"
            },
            tags=specs.dag_tags
            )

        with dag:
            if specs.sql:
                select_terms_from_db_task = PythonOperator(
                    task_id='select_terms_from_db',
                    python_callable=self.select_terms_from_db,
                    op_kwargs={
                        'sql': specs.sql,
                        'conn_id': specs.conn_id,
                        }
                )
                term_list = "{{ ti.xcom_pull(task_ids='select_terms_from_db') }}"

            exec_search_task = PythonOperator(
                task_id='exec_search',
                python_callable=self.perform_searches,
                op_kwargs={
                    'sources': specs.sources,
                    'territory_id': specs.territory_id,
                    'term_list': specs.terms or term_list,
                    'dou_sections': specs.dou_sections,
                    'search_date': specs.search_date,
                    'field': specs.field,
                    'is_exact_search': specs.is_exact_search,
                    'ignore_signature_match': specs.ignore_signature_match,
                    'force_rematch': specs.force_rematch,
                    'result_as_email': result_as_html(specs),
                    },
            )
            if specs.sql:
                select_terms_from_db_task >> exec_search_task # pylint: disable=pointless-statement

            has_matches_task = BranchPythonOperator(
                task_id='has_matches',
                python_callable=self.has_matches,
                op_kwargs={
                    'search_result': "{{ ti.xcom_pull(task_ids='exec_search') }}",
                    'skip_null': specs.skip_null,
                },
            )

            skip_notification_task = EmptyOperator(task_id='skip_notification')

            send_notification_task = PythonOperator(
                task_id='send_notification',
                python_callable=Notifier(specs).send_notification,
                op_kwargs={
                    'search_report':
                        "{{ ti.xcom_pull(task_ids='exec_search') }}",
                    'report_date': template_ano_mes_dia_trigger_local_time,
                    })

            exec_search_task >> has_matches_task # pylint: disable=pointless-statement
            has_matches_task >> [
                send_notification_task, skip_notification_task]

        return dag


    def perform_searches(
        self,
        sources,
        territory_id,
        term_list,
        dou_sections: List[str],
        search_date,
        field,
        is_exact_search: bool,
        ignore_signature_match: bool,
        force_rematch: bool,
        result_as_email: bool,
        **context) -> dict:
        """Performs the search in each source and merge the results
        """
        logging.info('Searching for: %s', ', '.join(term_list))
        logging.info(
            f'Trigger date: {get_trigger_date(context, local_time=True)}')

        if 'DOU' in sources:
            dou_result = self.searchers['DOU'].exec_search(
                term_list,
                dou_sections,
                search_date,
                field,
                is_exact_search,
                ignore_signature_match,
                force_rematch,
                get_trigger_date(context, local_time = True))

        if 'QD' in sources:
            qd_result = self.searchers['QD'].exec_search(
                territory_id,
                term_list,
                dou_sections,
                search_date,
                field,
                is_exact_search,
                ignore_signature_match,
                force_rematch,
                get_trigger_date(context, local_time = True),
                result_as_email)

        if 'DOU' in sources and 'QD' in sources:
            return merge_results(qd_result, dou_result)
        elif 'DOU' in sources:
            return dou_result
        else:
            return qd_result


    def has_matches(self, search_result: str, skip_null: bool) -> str:
        if skip_null:
            search_result = ast.literal_eval(search_result)
            items = ['contains' for k, v in search_result.items() if v]
            return 'send_notification' if items else 'skip_notification'
        else:
            return 'send_notification'

    def select_terms_from_db(self, sql: str, conn_id: str):
        """Queries the `sql` and return the list of terms that will be
        used later in the DOU search. The first column of the select
        must contain the terms to be searched. The second column, which
        is optional, is a classifier that will be used to group and sort
        the email report and the generated CSV.
        """
        conn_type = BaseHook.get_connection(conn_id).conn_type
        if conn_type == 'mssql':
            db_hook = MsSqlHook(conn_id)
        elif conn_type in ('postgresql', 'postgres'):
            db_hook = PostgresHook(conn_id)
        else:
            raise Exception('Tipo de banco de dados não suportado: ', conn_type)

        terms_df = db_hook.get_pandas_df(sql)
        # Remove unnecessary spaces and change null for ''
        terms_df = terms_df.applymap(
            lambda x: str.strip(x) if pd.notnull(x) else '')

        return terms_df.to_json(orient="columns")



SearchResult = Dict[str, Dict[str, List[dict]]]

def merge_results(result_1: SearchResult,
                  result_2: SearchResult) -> SearchResult:
    """Merge search results by group and term as keys"""
    return {
        group: _merge_dict(result_1.get(group, {}),
                           result_2.get(group, {}))
        for group in set((*result_1, *result_2))}


def _merge_dict(dict1, dict2):
    """Merge dictionaries and sum values of common keys"""
    dict3 = {**dict1, **dict2}
    for key, value in dict3.items():
        if key in dict1 and key in dict2:
                dict3[key] = value + dict1[key]
    return dict3

def result_as_html(specs: DAGConfig) -> bool:
    """Só utiliza resultado HTML apenas para email"""
    return specs.discord_webhook and specs.slack_webhook


# Run dag generation
DouDigestDagGenerator().generate_dags()
